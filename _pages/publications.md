---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---
<!-- 
## Recherche

Mes recherches se focalisent autour de deux thématiques :

### La fouille multimodale de bases d'images et de vidéos -->
<!-- Au cours de ma thèse, j'ai pu approfondir mes connaissances dans le domaine de l'indexation et de la recherche interactive d'images par le contenu au travers du projet IDEA (avec les professeurs Jean-Marc Ogier, Salvatore Tabbone et Alain Boucherde l’AUF). Pendant cette période, j'ai réalisé plusieurs recherches: 1) L'extraction des caractéristiques visuelles utilisé dans le domaine de la recherche par le contenu (signatures globales, signatures locales) ; 2) le développement de méthodes de recherche interactive d'images à court-terme ; 3) le développement de méthodes d'indexation et de recherche interactive d'images à long-terme.

Par la suite, dans le projet PEDIVHANDI avec Pr. Jean-Marc Ogier, mon objectif était d'explorer des méthodes d'indexation et de recherche multimodale de vidéos de cours. J'ai travaillé également avec différentes techniques  de traitement d'images : l'analyse de la mise en page, la séparation objets/fond, la détection de texte et de graphiques.   -->

<!-- Je travaille actuellement sur l'analyse multimodal des document images.  -->
<!-- 
+ L'indexation et la recherche d'images et de vidéos par le contenu
+ La recognition de texte et la reconnaissance d'objets
+ L'extraction des informations structurées  -->


<!-- ### L'apprentissage automatique  -->

<!-- Le laboratoire L3i, reconnu au niveau international sur l’analyse de document, a lancé en 2011 le projet e-bdthèque dont l’objectif est de valoriser les corpus de Bande Dessiné numérisées. J'ai commencé mon deuxieme postdoc dans l'équipe BD en 04/2016 (avec le prof. Jean-Christophe Burie, prof. Arnaud Revel et prof Karell Bertet). Dans ce contexte, j'ai travaillé principalement sur la détection automatique de cases de BD, de bulles, de dialogues, du texte, des personnages, des décors et l'analyse du contenu semantique. Je construis des algorithmes d’analyse d'images se basant sur l'apprentissage profond, en les combinant avec des algorithmes de traitement d'images classique, et priotiser le contexte semi-supervisé. -->

<!-- Depuis 04/2016, je suis intéréssé par la nouvelle tendance de l'apprentissage profond. En participant au Labcom SAIL du prof. Jean-Christophe Burie et au projet ECLATS avec Pr. Véronique Eglin, je commence à me tourner vers ce nouvelle approche qui ouvrent de nouvelles opportunités pour l'analyse de données. -->
<!-- de Bande Dessiné numérisées : la détection automatique de cases de BD, de bulles, de dialogues, du texte, des personnages et des décors.  -->
<!-- En pratique, je construis des algorithmes d’analyse d'images de BD se basant sur l'apprentissage profond, en les combinant avec des algorithmes de traitement d'images classique, et priotiser le contexte semi-supervisé, faiblement supervisé. -->

<!-- Je focalise sur deux axes de l'apprentissage profond. 

+ L'apprentissage profond avec des données et labels incomplètes et/ou bruyantes (le contexte semi-supervisé, faiblement supervisé, auto supervisé)
+ L'améliorer la performance des modèles IA en basant sur l'apprentissage multimodal et/ou multi-tâche -->


<!-- Je participe également au groupe de recherche du prof. Antoine Doucet sur la détection et la correction de textes OCR (Optical Character Recognition).  -->

<!-- ### Research Projects:

+ <b>The project [ECLATS](http://eclats.imag.fr/) (Extraction des Contenus GéoLinguistiques d'Atlas et Analyse Spatiale)</b> funded by the National Research Agency (ANR).
+ <b>The project [SAIL](https://sail.univ-lr.fr/) (Sequential Art Image Laboratory)</b> funded by the National Research Agency (ANR). -->
<!-- L'objectif est d'obtenir une description sémantique précise des albums, des pages et des cases tant au niveau géométrique (structure des pages) que textuel (compréhension du texte) et graphique (éléments visuels)des Bande Dessiné numérisées (Comics, Mangas et Webtoons). -->
<!-- + <b>The project [eBDthèque](http://ebdtheque.univ-lr.fr/) </b>  funded by the European Regional Development Fund (FEDER), as part of the Investing for the Future iiBD. -->
 <!-- dans le cadre de l’Investissement d’Avenir iiBD. L’objectif est de valoriser les corpus de Bande Dessiné numérisées. -->
<!-- + <b>The project Post-OCR:</b>  Errors detection and correction in texts OCR.  -->
<!-- la correction de textes OCR (Optical Character Recognition). Normalement, la reconnaissance de textes n’étant pas parfaite, une étape de post-traitement est nécessaire pour améliorer le texte.  -->
<!-- + <b>The project [PEDIVHANDI](http://pedivhandi.univ-lr.fr/) </b>  (Pédagogie et Diversité des Apprenants avec Priorité au Handicap) funded by the European Regional Development Fund (FEDER). -->
<!-- L’idée principale était d’analyser les contenus audiovisuels et documentaires par des techniques issues de l’analyse du signal et de l’image. Ces contenus multimédia sont le résultat de l’acquisition par différentes modalités : webcam issues de tablettes ou de PC, flux vidéo issus d’un système de visio-conférence… -->
<!-- + <b>The project IDEA </b>  (Images of natural Disasters from robot Exploration in urban Area) funded by the STIC-Asie program. -->
 <!-- se positionnait sur le thème du traitement d’images et l’extraction de contenu pour la gestion des informations collectées par des robots patrouillant dans les zones urbaines dans une situation de post-catastrophe. -->

<!-- ### Collaborations externes:
+ Pr. Antoine Doucet, Dr. Mikael Coustaty, Dr. Hai Nguyen (Université de La Rochelle, France)
+ Pr. Adam Jatowt (Kyoto University; Japan)
+ Pr. Huan Nguyen (Middlesex University)
+ Dr. Lili Jiang, Dr. Son Vu (Deep Data Mining group, Umeå University, Sweden)
+ Dr. Thanh Vu (Oracle, Australia)
+ Dr. Anh Bui (Purchease company)
+ Dr. Minh-Son Dao (NICT, Japan)
+ Dr. Kien Dao (MICA Institute, Vietnam) -->

<!-- ## Publications  -->

<!-- {% if author.googlescholar %} -->
<!-- La plupart de ces publications se trouvent également sur mon <u><a href="{{author.googlescholar}}">profil Google Scholar</a>.</u> -->
<!-- {% endif %} -->

<!-- <br>If you like the format of the preprints included here, see <u><a href="https://github.com/brenhinkeller/preprint-template.tex">preprint-template.tex</a></u> -->

---

<!-- <b>Working papers (3): </b>
<ol>
<li>Manga-MTL: multimodal multitask transfer learning for mangas (paper submitted)</li>
<li>Competition: Multimodal image document recognition for vietnamese receipts (paper accepted)</li>
<li>Competition: Multimodal Emotion Recognition on Comics scenes (in progress, 113 registered participants, 17 teams on leaderboard)</li>
</ol>
<br/> -->

<!-- --- -->
{% include base_path %}
<b>Journals (6): </b>
<br/>
  <ol>
    {% for post in site.publications reversed %}
      {% if post.journal %}
        {% include archive-single-cv.html %}
      {% endif %}
    {% endfor %}
  </ol>
<br/>

---
<b>Conferences (20):</b>
<br/>
<ol>{% for post in site.publications reversed %}
      {% if post.journal%}
        
      {% else %}
        {% include archive-single-cv.html %}
      {% endif %}
    {% endfor %}</ol>
<br/>
